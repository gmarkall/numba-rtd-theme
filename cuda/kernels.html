

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>3.2. Writing CUDA Kernels &mdash; Numba 0.49.0dev0+637.g3a659b257 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/numba-blue-icon-rgb.svg"/>
  
  
  
    <link rel="canonical" href="http://numba.pydata.org/numba-doc/latest/index.htmlcuda/kernels.html"/>
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.3. Memory management" href="memory.html" />
    <link rel="prev" title="3.1. Overview" href="overview.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #00A3E0" >
          

          
            <a href="../index.html" class="icon icon-home"> Numba
          

          
            
            <img src="../_static/numba-white-icon-rgb.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.49
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../user/index.html">1. User Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">2. Reference Manual</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">3. Numba for CUDA GPUs</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="overview.html">3.1. Overview</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">3.2. Writing CUDA Kernels</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">3.2.1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-declaration">3.2.2. Kernel declaration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kernel-invocation">3.2.3. Kernel invocation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#choosing-the-block-size">3.2.3.1. Choosing the block size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-dimensional-blocks-and-grids">3.2.3.2. Multi-dimensional blocks and grids</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#thread-positioning">3.2.4. Thread positioning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#absolute-positions">3.2.4.1. Absolute positions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#further-reading">3.2.4.2. Further Reading</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="memory.html">3.3. Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="device-functions.html">3.4. Writing Device Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="cudapysupported.html">3.5. Supported Python features in CUDA Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="intrinsics.html">3.6. Supported Atomic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="random.html">3.7. Random Number Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="device-management.html">3.8. Device management</a></li>
<li class="toctree-l2"><a class="reference internal" href="device-management.html#the-device-list">3.9. The Device List</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html">3.10. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="simulator.html">3.11. Debugging CUDA Python with the the CUDA Simulator</a></li>
<li class="toctree-l2"><a class="reference internal" href="reduction.html">3.12. GPU Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="ufunc.html">3.13. CUDA Ufuncs and Generalized Ufuncs</a></li>
<li class="toctree-l2"><a class="reference internal" href="ipc.html">3.14. Sharing CUDA Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda_array_interface.html">3.15. CUDA Array Interface (Version 2)</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html">3.16. CUDA Frequently Asked Questions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cuda-reference/index.html">4. CUDA Python Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../roc/index.html">5. Numba for AMD ROC GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extending/index.html">6. Extending Numba</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer/index.html">7. Developer Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../proposals/index.html">8. Numba Enhancement Proposals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">9. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">10. Release Notes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Numba</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">3. Numba for CUDA GPUs</a> &raquo;</li>
        
      <li>3.2. Writing CUDA Kernels</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/cuda/kernels.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="writing-cuda-kernels">
<h1>3.2. Writing CUDA Kernels<a class="headerlink" href="#writing-cuda-kernels" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>3.2.1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>CUDA has an execution model unlike the traditional sequential model used
for programming CPUs.  In CUDA, the code you write will be executed by
multiple threads at once (often hundreds or thousands).  Your solution will
be modeled by defining a thread hierarchy of <em>grid</em>, <em>blocks</em> and <em>threads</em>.</p>
<p>Numba’s CUDA support exposes facilities to declare and manage this
hierarchy of threads.  The facilities are largely similar to those
exposed by NVidia’s CUDA C language.</p>
<p>Numba also exposes three kinds of GPU memory: global <a class="reference internal" href="memory.html#cuda-device-memory"><span class="std std-ref">device memory</span></a> (the large, relatively slow
off-chip memory that’s connected to the GPU itself), on-chip
<a class="reference internal" href="memory.html#cuda-shared-memory"><span class="std std-ref">shared memory</span></a> and <a class="reference internal" href="memory.html#cuda-local-memory"><span class="std std-ref">local memory</span></a>.
For all but the simplest algorithms, it is important that you carefully
consider how to use and access memory in order to minimize bandwidth
requirements and contention.</p>
</div>
<div class="section" id="kernel-declaration">
<h2>3.2.2. Kernel declaration<a class="headerlink" href="#kernel-declaration" title="Permalink to this headline">¶</a></h2>
<p>A <em>kernel function</em> is a GPU function that is meant to be called from CPU
code (*).  It gives it two fundamental characteristics:</p>
<ul class="simple">
<li><p>kernels cannot explicitly return a value; all result data must be written
to an array passed to the function (if computing a scalar, you will
probably pass a one-element array);</p></li>
<li><p>kernels explicitly declare their thread hierarchy when called: i.e.
the number of thread blocks and the number of threads per block
(note that while a kernel is compiled once, it can be called multiple
times with different block sizes or grid sizes).</p></li>
</ul>
<p>At first sight, writing a CUDA kernel with Numba looks very much like
writing a <a class="reference internal" href="../glossary.html#term-jit-function"><span class="xref std std-term">JIT function</span></a> for the CPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">increment_by_one</span><span class="p">(</span><span class="n">an_array</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Increment all array elements by one.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># code elided here; read further for different implementations</span>
</pre></div>
</div>
<p>(*) Note: newer CUDA devices support device-side kernel launching; this feature
is called <em>dynamic parallelism</em> but Numba does not support it currently)</p>
</div>
<div class="section" id="kernel-invocation">
<span id="cuda-kernel-invocation"></span><h2>3.2.3. Kernel invocation<a class="headerlink" href="#kernel-invocation" title="Permalink to this headline">¶</a></h2>
<p>A kernel is typically launched in the following way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">threadsperblock</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">blockspergrid</span> <span class="o">=</span> <span class="p">(</span><span class="n">an_array</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="p">(</span><span class="n">threadsperblock</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">//</span> <span class="n">threadsperblock</span>
<span class="n">increment_by_one</span><span class="p">[</span><span class="n">blockspergrid</span><span class="p">,</span> <span class="n">threadsperblock</span><span class="p">](</span><span class="n">an_array</span><span class="p">)</span>
</pre></div>
</div>
<p>We notice two steps here:</p>
<ul class="simple">
<li><p>Instantiate the kernel proper, by specifying a number of blocks
(or “blocks per grid”), and a number of threads per block.  The product
of the two will give the total number of threads launched.  Kernel
instantiation is done by taking the compiled kernel function
(here <code class="docutils literal notranslate"><span class="pre">increment_by_one</span></code>) and indexing it with a tuple of integers.</p></li>
<li><p>Running the kernel, by passing it the input array (and any separate
output arrays if necessary).  By default, running a kernel is synchronous:
the function returns when the kernel has finished executing and the
data is synchronized back.</p></li>
</ul>
<div class="section" id="choosing-the-block-size">
<h3>3.2.3.1. Choosing the block size<a class="headerlink" href="#choosing-the-block-size" title="Permalink to this headline">¶</a></h3>
<p>It might seem curious to have a two-level hierarchy when declaring the
number of threads needed by a kernel.  The block size (i.e. number of
threads per block) is often crucial:</p>
<ul class="simple">
<li><p>On the software side, the block size determines how many threads
share a given area of <a class="reference internal" href="memory.html#cuda-shared-memory"><span class="std std-ref">shared memory</span></a>.</p></li>
<li><p>On the hardware side, the block size must be large enough for full
occupation of execution units; recommendations can be found in the
<a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide">CUDA C Programming Guide</a>.</p></li>
</ul>
</div>
<div class="section" id="multi-dimensional-blocks-and-grids">
<h3>3.2.3.2. Multi-dimensional blocks and grids<a class="headerlink" href="#multi-dimensional-blocks-and-grids" title="Permalink to this headline">¶</a></h3>
<p>To help deal with multi-dimensional arrays, CUDA allows you to specify
multi-dimensional blocks and grids.  In the example above, you could
make <code class="docutils literal notranslate"><span class="pre">blockspergrid</span></code> and <code class="docutils literal notranslate"><span class="pre">threadsperblock</span></code> tuples of one, two
or three integers.  Compared to 1D declarations of equivalent sizes,
this doesn’t change anything to the efficiency or behaviour of generated
code, but can help you write your algorithms in a more natural way.</p>
</div>
</div>
<div class="section" id="thread-positioning">
<h2>3.2.4. Thread positioning<a class="headerlink" href="#thread-positioning" title="Permalink to this headline">¶</a></h2>
<p>When running a kernel, the kernel function’s code is executed by every
thread once.  It therefore has to know which thread it is in, in order
to know which array element(s) it is responsible for (complex algorithms
may define more complex responsibilities, but the underlying principle
is the same).</p>
<p>One way is for the thread to determine its position in the grid and block
and manually compute the corresponding array position:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">increment_by_one</span><span class="p">(</span><span class="n">an_array</span><span class="p">):</span>
    <span class="c1"># Thread id in a 1D block</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="c1"># Block id in a 1D grid</span>
    <span class="n">ty</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span>
    <span class="c1"># Block width, i.e. number of threads per block</span>
    <span class="n">bw</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span>
    <span class="c1"># Compute flattened index inside the array</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">tx</span> <span class="o">+</span> <span class="n">ty</span> <span class="o">*</span> <span class="n">bw</span>
    <span class="k">if</span> <span class="n">pos</span> <span class="o">&lt;</span> <span class="n">an_array</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>  <span class="c1"># Check array boundaries</span>
        <span class="n">an_array</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless you are sure the block size and grid size is a divisor
of your array size, you <strong>must</strong> check boundaries as shown above.</p>
</div>
<p><a class="reference internal" href="../cuda-reference/kernel.html#numba.cuda.threadIdx" title="numba.cuda.threadIdx"><code class="xref py py-attr docutils literal notranslate"><span class="pre">threadIdx</span></code></a>, <a class="reference internal" href="../cuda-reference/kernel.html#numba.cuda.blockIdx" title="numba.cuda.blockIdx"><code class="xref py py-attr docutils literal notranslate"><span class="pre">blockIdx</span></code></a>, <a class="reference internal" href="../cuda-reference/kernel.html#numba.cuda.blockDim" title="numba.cuda.blockDim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">blockDim</span></code></a> and <a class="reference internal" href="../cuda-reference/kernel.html#numba.cuda.gridDim" title="numba.cuda.gridDim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">gridDim</span></code></a>
are special objects provided by the CUDA backend for the sole purpose of
knowing the geometry of the thread hierarchy and the position of the
current thread within that geometry.</p>
<p>These objects can be 1D, 2D or 3D, depending on how the kernel was
<a class="reference internal" href="#cuda-kernel-invocation"><span class="std std-ref">invoked</span></a>.  To access the value at each
dimension, use the <code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y</span></code> and <code class="docutils literal notranslate"><span class="pre">z</span></code> attributes of these objects,
respectively.</p>
<dl class="attribute">
<dt>
<code class="sig-prename descclassname">numba.cuda.</code><code class="sig-name descname">threadIdx</code></dt>
<dd><p>The thread indices in the current thread block.  For 1D blocks, the index
(given by the <code class="docutils literal notranslate"><span class="pre">x</span></code> attribute) is an integer spanning the range from 0
inclusive to <a class="reference internal" href="../cuda-reference/kernel.html#numba.cuda.blockDim" title="numba.cuda.blockDim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">numba.cuda.blockDim</span></code></a> exclusive.  A similar rule
exists for each dimension when more than one dimension is used.</p>
</dd></dl>

<dl class="attribute">
<dt>
<code class="sig-prename descclassname">numba.cuda.</code><code class="sig-name descname">blockDim</code></dt>
<dd><p>The shape of the block of threads, as declared when instantiating the
kernel.  This value is the same for all threads in a given kernel, even
if they belong to different blocks (i.e. each block is “full”).</p>
</dd></dl>

<dl class="attribute">
<dt>
<code class="sig-prename descclassname">numba.cuda.</code><code class="sig-name descname">blockIdx</code></dt>
<dd><p>The block indices in the grid of threads launched a kernel.  For a 1D grid,
the index (given by the <code class="docutils literal notranslate"><span class="pre">x</span></code> attribute) is an integer spanning the range
from 0 inclusive to <a class="reference internal" href="../cuda-reference/kernel.html#numba.cuda.gridDim" title="numba.cuda.gridDim"><code class="xref py py-attr docutils literal notranslate"><span class="pre">numba.cuda.gridDim</span></code></a> exclusive.  A similar rule
exists for each dimension when more than one dimension is used.</p>
</dd></dl>

<dl class="attribute">
<dt>
<code class="sig-prename descclassname">numba.cuda.</code><code class="sig-name descname">gridDim</code></dt>
<dd><p>The shape of the grid of blocks, i.e. the total number of blocks launched
by this kernel invocation, as declared when instantiating the kernel.</p>
</dd></dl>

<div class="section" id="absolute-positions">
<h3>3.2.4.1. Absolute positions<a class="headerlink" href="#absolute-positions" title="Permalink to this headline">¶</a></h3>
<p>Simple algorithms will tend to always use thread indices in the
same way as shown in the example above.  Numba provides additional facilities
to automate such calculations:</p>
<dl class="function">
<dt>
<code class="sig-prename descclassname">numba.cuda.</code><code class="sig-name descname">grid</code><span class="sig-paren">(</span><em class="sig-param">ndim</em><span class="sig-paren">)</span></dt>
<dd><p>Return the absolute position of the current thread in the entire
grid of blocks.  <em>ndim</em> should correspond to the number of dimensions
declared when instantiating the kernel.  If <em>ndim</em> is 1, a single integer
is returned.  If <em>ndim</em> is 2 or 3, a tuple of the given number of
integers is returned.</p>
</dd></dl>

<dl class="function">
<dt>
<code class="sig-prename descclassname">numba.cuda.</code><code class="sig-name descname">gridsize</code><span class="sig-paren">(</span><em class="sig-param">ndim</em><span class="sig-paren">)</span></dt>
<dd><p>Return the absolute size (or shape) in threads of the entire grid of
blocks.  <em>ndim</em> has the same meaning as in <a class="reference internal" href="../cuda-reference/kernel.html#numba.cuda.grid" title="numba.cuda.grid"><code class="xref py py-func docutils literal notranslate"><span class="pre">grid()</span></code></a> above.</p>
</dd></dl>

<p>With these functions, the incrementation example can become:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">increment_by_one</span><span class="p">(</span><span class="n">an_array</span><span class="p">):</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pos</span> <span class="o">&lt;</span> <span class="n">an_array</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">an_array</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>The same example for a 2D array and grid of threads would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@cuda</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">increment_a_2D_array</span><span class="p">(</span><span class="n">an_array</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">an_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">an_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
       <span class="n">an_array</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>Note the grid computation when instantiating the kernel must still be
done manually, for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">threadsperblock</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="n">blockspergrid_x</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">an_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">threadsperblock</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">blockspergrid_y</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">an_array</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">threadsperblock</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">blockspergrid</span> <span class="o">=</span> <span class="p">(</span><span class="n">blockspergrid_x</span><span class="p">,</span> <span class="n">blockspergrid_y</span><span class="p">)</span>
<span class="n">increment_a_2D_array</span><span class="p">[</span><span class="n">blockspergrid</span><span class="p">,</span> <span class="n">threadsperblock</span><span class="p">](</span><span class="n">an_array</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="further-reading">
<h3>3.2.4.2. Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h3>
<p>Please refer to the the <a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide">CUDA C Programming Guide</a> for a detailed discussion
of CUDA programming.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="memory.html" class="btn btn-neutral float-right" title="3.3. Memory management" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="overview.html" class="btn btn-neutral float-left" title="3.1. Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2012-2020, Anaconda, Inc. and others

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>