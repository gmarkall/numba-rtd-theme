

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>1.10. Automatic parallelization with @jit &mdash; Numba 0.49.0dev0+639.ga41f4317f.dirty documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/numba-blue-icon-rgb.svg"/>
  
  
  
    <link rel="canonical" href="http://numba.pydata.org/numba-doc/latest/index.htmluser/parallel.html"/>
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/rtd-overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.11. Using the @stencil decorator" href="stencil.html" />
    <link rel="prev" title="1.9. Compiling code ahead of time" href="pycc.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #00A3E0" >
          

          
            <a href="../index.html" class="icon icon-home"> Numba
          

          
            
            <img src="../_static/numba-white-icon-rgb.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.49
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">1. User Manual</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="5minguide.html">1.1. A ~5 minute guide to Numba</a></li>
<li class="toctree-l2"><a class="reference internal" href="overview.html">1.2. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="installing.html">1.3. Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit.html">1.4. Compiling Python code with <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="generated-jit.html">1.5. Flexible specializations with <code class="docutils literal notranslate"><span class="pre">&#64;generated_jit</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="vectorize.html">1.6. Creating Numpy universal functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="jitclass.html">1.7. Compiling Python classes with <code class="docutils literal notranslate"><span class="pre">&#64;jitclass</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="cfunc.html">1.8. Creating C callbacks with <code class="docutils literal notranslate"><span class="pre">&#64;cfunc</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="pycc.html">1.9. Compiling code ahead of time</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">1.10. Automatic parallelization with <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#supported-operations">1.10.1. Supported Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#explicit-parallel-loops">1.10.2. Explicit Parallel Loops</a></li>
<li class="toctree-l3"><a class="reference internal" href="#examples">1.10.3. Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="#diagnostics">1.10.4. Diagnostics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-parallel-diagnostics-report-sections">1.10.4.1. The parallel diagnostics report sections</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="stencil.html">1.11. Using the <code class="docutils literal notranslate"><span class="pre">&#64;stencil</span></code> decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="withobjmode.html">1.12. Callback into the Python Interpreter from within JIT’ed code</a></li>
<li class="toctree-l2"><a class="reference internal" href="jit-module.html">1.13. Automatic module jitting with <code class="docutils literal notranslate"><span class="pre">jit_module</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="performance-tips.html">1.14. Performance Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="threading-layer.html">1.15. The Threading Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="cli.html">1.16. Command line interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="troubleshoot.html">1.17. Troubleshooting and tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html">1.18. Frequently Asked Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html">1.19. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="talks.html">1.20. Talks and Tutorials</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">2. Reference Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda/index.html">3. Numba for CUDA GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda-reference/index.html">4. CUDA Python Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../roc/index.html">5. Numba for AMD ROC GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../extending/index.html">6. Extending Numba</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer/index.html">7. Developer Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="../proposals/index.html">8. Numba Enhancement Proposals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">9. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">10. Release Notes</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Numba</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">1. User Manual</a> &raquo;</li>
        
      <li>1.10. Automatic parallelization with <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code></li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/user/parallel.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="automatic-parallelization-with-jit">
<span id="numba-parallel"></span><h1>1.10. Automatic parallelization with <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code><a class="headerlink" href="#automatic-parallelization-with-jit" title="Permalink to this headline">¶</a></h1>
<p>Setting the <a class="reference internal" href="jit.html#parallel-jit-option"><span class="std std-ref">parallel</span></a> option for <a class="reference internal" href="../reference/jit-compilation.html#numba.jit" title="numba.jit"><code class="xref py py-func docutils literal notranslate"><span class="pre">jit()</span></code></a> enables
a Numba transformation pass that attempts to automatically parallelize and
perform other optimizations on (part of) a function. At the moment, this
feature only works on CPUs.</p>
<p>Some operations inside a user defined function, e.g. adding a scalar value to
an array, are known to have parallel semantics.  A user program may contain
many such operations and while each operation could be parallelized
individually, such an approach often has lackluster performance due to poor
cache behavior.  Instead, with auto-parallelization, Numba attempts to
identify such operations in a user program, and fuse adjacent ones together,
to form one or more kernels that are automatically run in parallel.
The process is fully automated without modifications to the user program,
which is in contrast to Numba’s <a class="reference internal" href="../reference/jit-compilation.html#numba.vectorize" title="numba.vectorize"><code class="xref py py-func docutils literal notranslate"><span class="pre">vectorize()</span></code></a> or
<a class="reference internal" href="../reference/jit-compilation.html#numba.guvectorize" title="numba.guvectorize"><code class="xref py py-func docutils literal notranslate"><span class="pre">guvectorize()</span></code></a> mechanism, where manual effort is required
to create parallel kernels.</p>
<div class="section" id="supported-operations">
<span id="numba-parallel-supported"></span><h2>1.10.1. Supported Operations<a class="headerlink" href="#supported-operations" title="Permalink to this headline">¶</a></h2>
<p>In this section, we give a list of all the array operations that have
parallel semantics and for which we attempt to parallelize.</p>
<ol class="arabic">
<li><p>All numba array operations that are supported by <a class="reference internal" href="../developer/rewrites.html#case-study-array-expressions"><span class="std std-ref">Case study: Array Expressions</span></a>,
which include common arithmetic functions between Numpy arrays, and between
arrays and scalars, as well as Numpy ufuncs. They are often called
<cite>element-wise</cite> or <cite>point-wise</cite> array operations:</p>
<blockquote>
<div><ul class="simple">
<li><p>unary operators: <code class="docutils literal notranslate"><span class="pre">+</span></code> <code class="docutils literal notranslate"><span class="pre">-</span></code> <code class="docutils literal notranslate"><span class="pre">~</span></code></p></li>
<li><p>binary operators: <code class="docutils literal notranslate"><span class="pre">+</span></code> <code class="docutils literal notranslate"><span class="pre">-</span></code> <code class="docutils literal notranslate"><span class="pre">*</span></code> <code class="docutils literal notranslate"><span class="pre">/</span></code> <code class="docutils literal notranslate"><span class="pre">/?</span></code> <code class="docutils literal notranslate"><span class="pre">%</span></code> <code class="docutils literal notranslate"><span class="pre">|</span></code> <code class="docutils literal notranslate"><span class="pre">&gt;&gt;</span></code> <code class="docutils literal notranslate"><span class="pre">^</span></code> <code class="docutils literal notranslate"><span class="pre">&lt;&lt;</span></code> <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> <code class="docutils literal notranslate"><span class="pre">**</span></code> <code class="docutils literal notranslate"><span class="pre">//</span></code></p></li>
<li><p>comparison operators: <code class="docutils literal notranslate"><span class="pre">==</span></code> <code class="docutils literal notranslate"><span class="pre">!=</span></code> <code class="docutils literal notranslate"><span class="pre">&lt;</span></code> <code class="docutils literal notranslate"><span class="pre">&lt;=</span></code> <code class="docutils literal notranslate"><span class="pre">&gt;</span></code> <code class="docutils literal notranslate"><span class="pre">&gt;=</span></code></p></li>
<li><p><a class="reference internal" href="../reference/numpysupported.html#supported-ufuncs"><span class="std std-ref">Numpy ufuncs</span></a> that are supported in <a class="reference internal" href="../glossary.html#term-nopython-mode"><span class="xref std std-term">nopython mode</span></a>.</p></li>
<li><p>User defined <a class="reference internal" href="../reference/jit-compilation.html#numba.DUFunc" title="numba.DUFunc"><code class="xref py py-class docutils literal notranslate"><span class="pre">DUFunc</span></code></a> through <a class="reference internal" href="../reference/jit-compilation.html#numba.vectorize" title="numba.vectorize"><code class="xref py py-func docutils literal notranslate"><span class="pre">vectorize()</span></code></a>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Numpy reduction functions <code class="docutils literal notranslate"><span class="pre">sum</span></code>, <code class="docutils literal notranslate"><span class="pre">prod</span></code>, <code class="docutils literal notranslate"><span class="pre">min</span></code>, <code class="docutils literal notranslate"><span class="pre">max</span></code>, <code class="docutils literal notranslate"><span class="pre">argmin</span></code>,
and <code class="docutils literal notranslate"><span class="pre">argmax</span></code>. Also, array math functions <code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">var</span></code>, and <code class="docutils literal notranslate"><span class="pre">std</span></code>.</p></li>
<li><p>Numpy array creation functions <code class="docutils literal notranslate"><span class="pre">zeros</span></code>, <code class="docutils literal notranslate"><span class="pre">ones</span></code>, <code class="docutils literal notranslate"><span class="pre">arange</span></code>, <code class="docutils literal notranslate"><span class="pre">linspace</span></code>,
and several random functions (rand, randn, ranf, random_sample, sample,
random, standard_normal, chisquare, weibull, power, geometric, exponential,
poisson, rayleigh, normal, uniform, beta, binomial, f, gamma, lognormal,
laplace, randint, triangular).</p></li>
<li><p>Numpy <code class="docutils literal notranslate"><span class="pre">dot</span></code> function between a matrix and a vector, or two vectors.
In all other cases, Numba’s default implementation is used.</p></li>
<li><p>Multi-dimensional arrays are also supported for the above operations
when operands have matching dimension and size. The full semantics of
Numpy broadcast between arrays with mixed dimensionality or size is
not supported, nor is the reduction across a selected dimension.</p></li>
<li><p>Array assignment in which the target is an array selection using a slice
or a boolean array, and the value being assigned is either a scalar or
another selection where the slice range or bitarray are inferred to be
compatible.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">reduce</span></code> operator of <code class="docutils literal notranslate"><span class="pre">functools</span></code> is supported for specifying parallel
reductions on 1D Numpy arrays but the initial value argument is mandatory.</p></li>
</ol>
</div>
<div class="section" id="explicit-parallel-loops">
<span id="numba-prange"></span><h2>1.10.2. Explicit Parallel Loops<a class="headerlink" href="#explicit-parallel-loops" title="Permalink to this headline">¶</a></h2>
<p>Another feature of the code transformation pass (when <code class="docutils literal notranslate"><span class="pre">parallel=True</span></code>) is
support for explicit parallel loops. One can use Numba’s <code class="docutils literal notranslate"><span class="pre">prange</span></code> instead of
<code class="docutils literal notranslate"><span class="pre">range</span></code> to specify that a loop can be parallelized. The user is required to
make sure that the loop does not have cross iteration dependencies except for
supported reductions.</p>
<p>A reduction is inferred automatically if a variable is updated by a binary
function/operator using its previous value in the loop body. The initial value
of the reduction is inferred automatically for the <code class="docutils literal notranslate"><span class="pre">+=</span></code>, <code class="docutils literal notranslate"><span class="pre">-=</span></code>,  <code class="docutils literal notranslate"><span class="pre">*=</span></code>,
and <code class="docutils literal notranslate"><span class="pre">/=</span></code> operators.
For other functions/operators, the reduction variable should hold the identity
value right before entering the <code class="docutils literal notranslate"><span class="pre">prange</span></code> loop.  Reductions in this manner
are supported for scalars and for arrays of arbitrary dimensions.</p>
<p>The example below demonstrates a parallel loop with a
reduction (<code class="docutils literal notranslate"><span class="pre">A</span></code> is a one-dimensional Numpy array):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>

<span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">prange_test</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Without &quot;parallel=True&quot; in the jit-decorator</span>
    <span class="c1"># the prange statement is equivalent to range</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">s</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">s</span>
</pre></div>
</div>
<p>The following example demonstrates a product reduction on a two-dimensional array:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">two_d_array_reduction_prod</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">shp</span> <span class="o">=</span> <span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span>
    <span class="n">result1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shp</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int_</span><span class="p">)</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">result1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">result1</span> <span class="o">*=</span> <span class="n">tmp</span>

    <span class="k">return</span> <span class="n">result1</span>
</pre></div>
</div>
<p>Care should be taken, however, when reducing into slices or elements of an array
if the elements specified by the slice or index are written to simultaneously by
multiple parallel threads. The compiler may not detect such cases and then a race condition
would occur.</p>
<p>The following example demonstrates such a case where a race condition in the execution of the
parallel for-loop results in an incorrect return value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">prange_wrong_result</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># accumulating into the same element of `y` from different</span>
        <span class="c1"># parallel iterations of the loop results in a race condition</span>
        <span class="n">y</span><span class="p">[:]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
<p>as does the following example where the accumulating element is explicitly specified:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">prange_wrong_result</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># accumulating into the same element of `y` from different</span>
        <span class="c1"># parallel iterations of the loop results in a race condition</span>
        <span class="n">y</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">4</span><span class="p">]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
<p>whereas performing a whole array reduction is fine:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">prange_ok_result_whole_arr</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
<p>as is creating a slice reference outside of the parallel reduction loop:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span><span class="p">,</span> <span class="n">prange</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">prange_ok_result_outer_slice</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
<div class="section" id="examples">
<h2>1.10.3. Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<p>In this section, we give an example of how this feature helps
parallelize Logistic Regression:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@numba</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">nopython</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">logistic_regression</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">iterations</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">w</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(((</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">Y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)))</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">Y</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">w</span>
</pre></div>
</div>
<p>We will not discuss details of the algorithm, but instead focus on how
this program behaves with auto-parallelization:</p>
<ol class="arabic simple">
<li><p>Input <code class="docutils literal notranslate"><span class="pre">Y</span></code> is a vector of size <code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">X</span></code> is an <code class="docutils literal notranslate"><span class="pre">N</span> <span class="pre">x</span> <span class="pre">D</span></code> matrix,
and <code class="docutils literal notranslate"><span class="pre">w</span></code> is a vector of size <code class="docutils literal notranslate"><span class="pre">D</span></code>.</p></li>
<li><p>The function body is an iterative loop that updates variable <code class="docutils literal notranslate"><span class="pre">w</span></code>.
The loop body consists of a sequence of vector and matrix operations.</p></li>
<li><p>The inner <code class="docutils literal notranslate"><span class="pre">dot</span></code> operation produces a vector of size <code class="docutils literal notranslate"><span class="pre">N</span></code>, followed by a
sequence of arithmetic operations either between a scalar and vector of
size <code class="docutils literal notranslate"><span class="pre">N</span></code>, or two vectors both of size <code class="docutils literal notranslate"><span class="pre">N</span></code>.</p></li>
<li><p>The outer <code class="docutils literal notranslate"><span class="pre">dot</span></code> produces a vector of size <code class="docutils literal notranslate"><span class="pre">D</span></code>, followed by an inplace
array subtraction on variable <code class="docutils literal notranslate"><span class="pre">w</span></code>.</p></li>
<li><p>With auto-parallelization, all operations that produce array of size
<code class="docutils literal notranslate"><span class="pre">N</span></code> are fused together to become a single parallel kernel. This includes
the inner <code class="docutils literal notranslate"><span class="pre">dot</span></code> operation and all point-wise array operations following it.</p></li>
<li><p>The outer <code class="docutils literal notranslate"><span class="pre">dot</span></code> operation produces a result array of different dimension,
and is not fused with the above kernel.</p></li>
</ol>
<p>Here, the only thing required to take advantage of parallel hardware is to set
the <a class="reference internal" href="jit.html#parallel-jit-option"><span class="std std-ref">parallel</span></a> option for <a class="reference internal" href="../reference/jit-compilation.html#numba.jit" title="numba.jit"><code class="xref py py-func docutils literal notranslate"><span class="pre">jit()</span></code></a>, with no
modifications to the <code class="docutils literal notranslate"><span class="pre">logistic_regression</span></code> function itself.  If we were to
give an equivalence parallel implementation using <a class="reference internal" href="../reference/jit-compilation.html#numba.guvectorize" title="numba.guvectorize"><code class="xref py py-func docutils literal notranslate"><span class="pre">guvectorize()</span></code></a>,
it would require a pervasive change that rewrites the code to extract kernel
computation that can be parallelized, which was both tedious and challenging.</p>
</div>
<div class="section" id="diagnostics">
<span id="numba-parallel-diagnostics"></span><h2>1.10.4. Diagnostics<a class="headerlink" href="#diagnostics" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At present not all parallel transforms and functions can be tracked
through the code generation process. Occasionally diagnostics about
some loops or transforms may be missing.</p>
</div>
<p>The <a class="reference internal" href="jit.html#parallel-jit-option"><span class="std std-ref">parallel</span></a> option for <a class="reference internal" href="../reference/jit-compilation.html#numba.jit" title="numba.jit"><code class="xref py py-func docutils literal notranslate"><span class="pre">jit()</span></code></a> can produce
diagnostic information about the transforms undertaken in automatically
parallelizing the decorated code. This information can be accessed in two ways,
the first is by setting the environment variable
<span class="target" id="index-0"></span><a class="reference internal" href="../reference/envvars.html#envvar-NUMBA_PARALLEL_DIAGNOSTICS"><code class="xref std std-envvar docutils literal notranslate"><span class="pre">NUMBA_PARALLEL_DIAGNOSTICS</span></code></a>, the second is by calling
<a class="reference internal" href="../reference/jit-compilation.html#Dispatcher.parallel_diagnostics" title="Dispatcher.parallel_diagnostics"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parallel_diagnostics()</span></code></a>, both methods give the same information
and print to <code class="docutils literal notranslate"><span class="pre">STDOUT</span></code>. The level of verbosity in the diagnostic information is
controlled by an integer argument of value between 1 and 4 inclusive, 1 being
the least verbose and 4 the most. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">acc</span> <span class="o">+=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">acc</span>

<span class="n">test</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

<span class="n">test</span><span class="o">.</span><span class="n">parallel_diagnostics</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>produces:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>================================================================================
======= Parallel Accelerator Optimizing:  Function test, example.py (4)  =======
================================================================================


Parallel loop listing for  Function test, example.py (4)
--------------------------------------|loop #ID
@njit(parallel=True)                  |
def test(x):                          |
    n = x.shape[0]                    |
    a = np.sin(x)---------------------| #0
    b = np.cos(a * a)-----------------| #1
    acc = 0                           |
    for i in prange(n - 2):-----------| #3
        for j in prange(n - 1):-------| #2
            acc += b[i] + b[j + 1]    |
    return acc                        |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Trying to fuse loops #0 and #1:
    - fusion succeeded: parallel for-loop #1 is fused into for-loop #0.
Trying to fuse loops #0 and #3:
    - fusion failed: loop dimension mismatched in axis 0. slice(0, x_size0.1, 1)
!= slice(0, $40.4, 1)
----------------------------- Before Optimization ------------------------------
Parallel region 0:
+--0 (parallel)
+--1 (parallel)


Parallel region 1:
+--3 (parallel)
+--2 (parallel)


--------------------------------------------------------------------------------
------------------------------ After Optimization ------------------------------
Parallel region 0:
+--0 (parallel, fused with loop(s): 1)


Parallel region 1:
+--3 (parallel)
+--2 (serial)



Parallel region 0 (loop #0) had 1 loop(s) fused.

Parallel region 1 (loop #3) had 0 loop(s) fused and 1 loop(s) serialized as part
of the larger parallel loop (#3).
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------

---------------------------Loop invariant code motion---------------------------

Instruction hoisting:
loop #0:
Failed to hoist the following:
    dependency: $arg_out_var.10 = getitem(value=x, index=$parfor__index_5.99)
    dependency: $0.6.11 = getattr(value=$0.5, attr=sin)
    dependency: $expr_out_var.9 = call $0.6.11($arg_out_var.10, func=$0.6.11, args=[Var($arg_out_var.10, example.py (7))], kws=(), vararg=None)
    dependency: $arg_out_var.17 = $expr_out_var.9 * $expr_out_var.9
    dependency: $0.10.20 = getattr(value=$0.9, attr=cos)
    dependency: $expr_out_var.16 = call $0.10.20($arg_out_var.17, func=$0.10.20, args=[Var($arg_out_var.17, example.py (8))], kws=(), vararg=None)
loop #3:
Has the following hoisted:
    $const58.3 = const(int, 1)
    $58.4 = _n_23 - $const58.3
--------------------------------------------------------------------------------
</pre></div>
</div>
<p>To aid users unfamiliar with the transforms undertaken when the
<a class="reference internal" href="jit.html#parallel-jit-option"><span class="std std-ref">parallel</span></a> option is used, and to assist in the understanding of
the subsequent sections, the following definitions are provided:</p>
<ul>
<li><dl class="simple">
<dt>Loop fusion</dt><dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Loop_fission_and_fusion">Loop fusion</a> is a
technique whereby loops with equivalent bounds may be combined under certain
conditions to produce a loop with a larger body (aiming to improve data
locality).</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Loop serialization</dt><dd><p>Loop serialization occurs when any number of <code class="docutils literal notranslate"><span class="pre">prange</span></code> driven loops are
present inside another <code class="docutils literal notranslate"><span class="pre">prange</span></code> driven loop. In this case the outermost
of all the <code class="docutils literal notranslate"><span class="pre">prange</span></code> loops executes in parallel and any inner <code class="docutils literal notranslate"><span class="pre">prange</span></code>
loops (nested or otherwise) are treated as standard <code class="docutils literal notranslate"><span class="pre">range</span></code> based loops.
Essentially, nested parallelism does not occur.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Loop invariant code motion</dt><dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Loop-invariant_code_motion">Loop invariant code motion</a> is an
optimization technique that analyses a loop to look for statements that can
be moved outside the loop body without changing the result of executing the
loop, these statements are then “hoisted” out of the loop to save repeated
computation.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>Allocation hoisting</dt><dd><p>Allocation hoisting is a specialized case of loop invariant code motion that
is possible due to the design of some common NumPy allocation methods.
Explanation of this technique is best driven by an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span> <span class="c1"># &lt;--- Allocate a temporary array with np.zeros()</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">temp</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

    <span class="c1"># ...do something with temp</span>
</pre></div>
</div>
<p>internally, this is transformed to approximately the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span> <span class="c1"># &lt;--- np.zeros() is rewritten as np.empty()</span>
        <span class="n">temp</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>               <span class="c1"># &lt;--- and then a zero initialisation</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">temp</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

    <span class="c1"># ...do something with temp</span>
</pre></div>
</div>
<p>then after hoisting:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span> <span class="c1"># &lt;--- allocation is hoisted as a loop invariant as `np.empty` is considered pure</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">temp</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>           <span class="c1"># &lt;--- this remains as assignment is a side effect</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">temp</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>

    <span class="c1"># ...do something with temp</span>
</pre></div>
</div>
<p>it can be seen that the <code class="docutils literal notranslate"><span class="pre">np.zeros</span></code> allocation is split into an allocation
and an assignment, and then the allocation is hoisted out of the loop in
<code class="docutils literal notranslate"><span class="pre">i</span></code>, this producing more efficient code as the allocation only occurs
once.</p>
</dd>
</dl>
</li>
</ul>
<div class="section" id="the-parallel-diagnostics-report-sections">
<h3>1.10.4.1. The parallel diagnostics report sections<a class="headerlink" href="#the-parallel-diagnostics-report-sections" title="Permalink to this headline">¶</a></h3>
<p>The report is split into the following sections:</p>
<ol class="arabic">
<li><dl>
<dt>Code annotation</dt><dd><p>This is the first section and contains the source code of the decorated
function with loops that have parallel semantics identified and enumerated.
The <code class="docutils literal notranslate"><span class="pre">loop</span> <span class="pre">#ID</span></code> column on the right of the source code lines up with
identified parallel loops. From the example, <code class="docutils literal notranslate"><span class="pre">#0</span></code> is <code class="docutils literal notranslate"><span class="pre">np.sin</span></code>, <code class="docutils literal notranslate"><span class="pre">#1</span></code>
is <code class="docutils literal notranslate"><span class="pre">np.cos</span></code> and <code class="docutils literal notranslate"><span class="pre">#2</span></code> and <code class="docutils literal notranslate"><span class="pre">#3</span></code> are <code class="docutils literal notranslate"><span class="pre">prange()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Parallel</span> <span class="n">loop</span> <span class="n">listing</span> <span class="k">for</span>  <span class="n">Function</span> <span class="n">test</span><span class="p">,</span> <span class="n">example</span><span class="o">.</span><span class="n">py</span> <span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="o">--------------------------------------|</span><span class="n">loop</span> <span class="c1">#ID</span>
<span class="nd">@njit</span><span class="p">(</span><span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>                  <span class="o">|</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>                          <span class="o">|</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>                    <span class="o">|</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">---------------------|</span> <span class="c1">#0</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span><span class="o">-----------------|</span> <span class="c1">#1</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>                           <span class="o">|</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span><span class="o">-----------|</span> <span class="c1">#3</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span><span class="o">-------|</span> <span class="c1">#2</span>
            <span class="n">acc</span> <span class="o">+=</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>    <span class="o">|</span>
    <span class="k">return</span> <span class="n">acc</span>                        <span class="o">|</span>
</pre></div>
</div>
<p>It is worth noting that the loop IDs are enumerated in the order they are
discovered which is not necessarily the same order as present in the source.
Further, it should also be noted that the parallel transforms use a static
counter for loop ID indexing. As a consequence it is possible for the loop
ID index to not start at 0 due to use of the same counter for internal
optimizations/transforms taking place that are invisible to the user.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>Fusing loops</dt><dd><p>This section describes the attempts made at fusing discovered
loops noting which succeeded and which failed. In the case of failure to
fuse a reason is given (e.g. dependency on other data). From the example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Trying to fuse loops #0 and #1:
    - fusion succeeded: parallel for-loop #1 is fused into for-loop #0.
Trying to fuse loops #0 and #3:
    - fusion failed: loop dimension mismatched in axis 0. slice(0, x_size0.1, 1)
!= slice(0, $40.4, 1)
</pre></div>
</div>
<p>It can be seen that fusion of loops <code class="docutils literal notranslate"><span class="pre">#0</span></code> and <code class="docutils literal notranslate"><span class="pre">#1</span></code> was attempted and this
succeeded (both are based on the same dimensions of <code class="docutils literal notranslate"><span class="pre">x</span></code>). Following the
successful fusion of <code class="docutils literal notranslate"><span class="pre">#0</span></code> and <code class="docutils literal notranslate"><span class="pre">#1</span></code>, fusion was attempted between <code class="docutils literal notranslate"><span class="pre">#0</span></code>
(now including the fused <code class="docutils literal notranslate"><span class="pre">#1</span></code> loop) and <code class="docutils literal notranslate"><span class="pre">#3</span></code>. This fusion failed because
there is a loop dimension mismatch, <code class="docutils literal notranslate"><span class="pre">#0</span></code> is size <code class="docutils literal notranslate"><span class="pre">x.shape</span></code> whereas
<code class="docutils literal notranslate"><span class="pre">#3</span></code> is size <code class="docutils literal notranslate"><span class="pre">x.shape[0]</span> <span class="pre">-</span> <span class="pre">2</span></code>.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>Before Optimization</dt><dd><p>This section shows the structure of the parallel regions in the code before
any optimization has taken place, but with loops associated with their final
parallel region (this is to make before/after optimization output directly
comparable). Multiple parallel regions may exist if there are loops which
cannot be fused, in this case code within each region will execute in
parallel, but each parallel region will run sequentially. From the example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Parallel region 0:
+--0 (parallel)
+--1 (parallel)


Parallel region 1:
+--3 (parallel)
+--2 (parallel)
</pre></div>
</div>
<p>As alluded to by the <cite>Fusing loops</cite> section, there are necessarily two
parallel regions in the code. The first contains loops <code class="docutils literal notranslate"><span class="pre">#0</span></code> and <code class="docutils literal notranslate"><span class="pre">#1</span></code>,
the second contains <code class="docutils literal notranslate"><span class="pre">#3</span></code> and <code class="docutils literal notranslate"><span class="pre">#2</span></code>, all loops are marked <code class="docutils literal notranslate"><span class="pre">parallel</span></code> as
no optimization has taken place yet.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>After Optimization</dt><dd><p>This section shows the structure of the parallel regions in the code after
optimization has taken place. Again, parallel regions are enumerated with
their corresponding loops but this time loops which are fused or serialized
are noted and a summary is presented. From the example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Parallel region 0:
+--0 (parallel, fused with loop(s): 1)


Parallel region 1:
+--3 (parallel)
   +--2 (serial)

Parallel region 0 (loop #0) had 1 loop(s) fused.

Parallel region 1 (loop #3) had 0 loop(s) fused and 1 loop(s) serialized as part
of the larger parallel loop (#3).
</pre></div>
</div>
<p>It can be noted that parallel region 0 contains loop <code class="docutils literal notranslate"><span class="pre">#0</span></code> and, as seen in
the <cite>fusing loops</cite> section, loop <code class="docutils literal notranslate"><span class="pre">#1</span></code> is fused into loop <code class="docutils literal notranslate"><span class="pre">#0</span></code>. It can
also be noted that parallel region 1 contains loop <code class="docutils literal notranslate"><span class="pre">#3</span></code> and that loop
<code class="docutils literal notranslate"><span class="pre">#2</span></code> (the inner <code class="docutils literal notranslate"><span class="pre">prange()</span></code>) has been serialized for execution in the
body of loop <code class="docutils literal notranslate"><span class="pre">#3</span></code>.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>Loop invariant code motion</dt><dd><p>This section shows for each loop, after optimization has occurred:</p>
<ul class="simple">
<li><p>the instructions that failed to be hoisted and the reason for failure
(dependency/impure).</p></li>
<li><p>the instructions that were hoisted.</p></li>
<li><p>any allocation hoisting that may have occurred.</p></li>
</ul>
<p>From the example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Instruction hoisting:
loop #0:
Failed to hoist the following:
    dependency: $arg_out_var.10 = getitem(value=x, index=$parfor__index_5.99)
    dependency: $0.6.11 = getattr(value=$0.5, attr=sin)
    dependency: $expr_out_var.9 = call $0.6.11($arg_out_var.10, func=$0.6.11, args=[Var($arg_out_var.10, example.py (7))], kws=(), vararg=None)
    dependency: $arg_out_var.17 = $expr_out_var.9 * $expr_out_var.9
    dependency: $0.10.20 = getattr(value=$0.9, attr=cos)
    dependency: $expr_out_var.16 = call $0.10.20($arg_out_var.17, func=$0.10.20, args=[Var($arg_out_var.17, example.py (8))], kws=(), vararg=None)
loop #3:
Has the following hoisted:
    $const58.3 = const(int, 1)
    $58.4 = _n_23 - $const58.3
</pre></div>
</div>
<p>The first thing to note is that this information is for advanced users as it
refers to the <a class="reference internal" href="../glossary.html#term-numba-ir"><span class="xref std std-term">Numba IR</span></a> of the function being transformed. As an
example, the expression <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">*</span> <span class="pre">a</span></code> in the example source partly translates to
the expression <code class="docutils literal notranslate"><span class="pre">$arg_out_var.17</span> <span class="pre">=</span> <span class="pre">$expr_out_var.9</span> <span class="pre">*</span> <span class="pre">$expr_out_var.9</span></code> in
the IR, this clearly cannot be hoisted out of <code class="docutils literal notranslate"><span class="pre">loop</span> <span class="pre">#0</span></code> because it is not
loop invariant! Whereas in <code class="docutils literal notranslate"><span class="pre">loop</span> <span class="pre">#3</span></code>, the expression
<code class="docutils literal notranslate"><span class="pre">$const58.3</span> <span class="pre">=</span> <span class="pre">const(int,</span> <span class="pre">1)</span></code> comes from the source <code class="docutils literal notranslate"><span class="pre">b[j</span> <span class="pre">+</span> <span class="pre">1]</span></code>, the
number <code class="docutils literal notranslate"><span class="pre">1</span></code> is clearly a constant and so can be hoisted out of the loop.</p>
</dd>
</dl>
</li>
</ol>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="jit.html#parallel-jit-option"><span class="std std-ref">parallel</span></a>, <a class="reference internal" href="faq.html#parallel-faqs"><span class="std std-ref">Parallel FAQs</span></a></p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="stencil.html" class="btn btn-neutral float-right" title="1.11. Using the @stencil decorator" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="pycc.html" class="btn btn-neutral float-left" title="1.9. Compiling code ahead of time" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2012-2020, Anaconda, Inc. and others

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>